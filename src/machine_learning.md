
# Machine learning

## Generally about AI

- https://medium.com/@cdixon/what-s-next-in-computing-e54b870b80cc
- https://medium.com/machine-intelligence-report/the-current-state-of-machine-intelligence-2-0-a9e0bab95511
- Hello World - Machine Learning Recipes #1 https://www.youtube.com/watch?v=cKxRvEZd3Mw
- IQ as a service http://techcrunch.com/2016/05/07/the-next-stop-on-the-road-to-revolution-is-ambient-intelligence/
- AI is today synonmous with algorithm that are automatically tuned given new data, and don't require changing their source code to perform better https://news.ycombinator.com/item?id=11651863
- Machine Learning is where you can infer something (usually classify or extrapolate) about future data based on present data/examples and this inference has had no human input determining it https://news.ycombinator.com/item?id=11651225
- But calling machine learning algorithms just algorithms is not helpful at all. I had to work through a few books and courses to get a decent understanding of ML despite the fact that I had a CS degree and a good understanding of discrete algorithms. In short, the things that are put on top of the technology stack do not reduce to the layers below in a useful way. https://news.ycombinator.com/item?id=11650747
- Is this just a huge Monte Carlo simulation machine for generating loads of input setups, run AI many times to see which setup was "best"? https://news.ycombinator.com/item?id=11651005
- As Mehana says, Facebook trains and tests about 300,000 machine learning models each month. AutoML can then use the results of these tests to train another machine learning model that can optimize the training of machine learning models. Yes, that can be a hard thing to wrap your head around. Mehanna compares it to Inception. But it works. The system can automatically chooses algorithms and parameters that are likely to work. “It can almost predict the result before the training,” Mehanna says. http://www.wired.com/2016/05/facebook-trying-create-ai-can-create-ai/
- https://medium.com/ai-revolution/ai-revolution-101-8dce1d9cb62d

## About ML

> In machine learning, computers apply statistical learning techniques to automatically identify patterns in data. These techniques can be used to make highly accurate predictions.
> - http://www.r2d3.us/visual-intro-to-machine-learning-part-1/?lang=en

> While most of the core algorithms that drive machine learning have been around for decades, what has magnified its promise so dramatically in recent years is the extraordinary growth of the two fuels that power these algorithms – data and computing power.
> - http://techcrunch.com/2016/06/02/the-barbell-effect-of-machine-learning/

> to make predictions about other, unseen data.
>
> how can you speak of machine learning if the metric is fixed before seeing any data? PageRank is human learning: people look at a bunch of pages they want categorized, figure out an intuitively appealing way to categorize them, and then encode those rules in an algorithm
> Edit: after thinking about this a bit more, I guess you could in fact think of e.g. k-means clustering as just a very advanced form of descriptive statistics, perhaps not fundamentally different from calculating a mean or a kernel density estimate. And in that sense PageRank would be unsupervised learning too, but it still feels to me like that's obscuring rather than clarifying how it works?
> - https://news.ycombinator.com/item?id=11837475

> I don't believe in "everyone should work on machine learning". I worked on several deep learning models but I don't really like it. It is a very different job than software engineering in my opinion. ML is more about gathering data and tuning the models as opposed to building stuff. I have spent months working on models and barely wrote any code. It is more efficient to have ML experts focus on the modeling and software engineers use the model.
>
> The sudden breakthrough in the mid-2000s is IMO still not fully understood - and parts of it may have been very accidental. Initially, it was thought that pre-training was the big breakthrough, but it is quite unclear what the big breakthrough was. It could be that simply the increase of data / compute sizes and the switch to minibatch-SGD explains why modern DNNs generalize well
>
> we need better sensors more than greater amounts of the same data available now. We need better hypotheses which lead to better ideas of where to look and what to look for. In general, ML can't help with that. Until we better imagine how the mechanism might work, our questions remain too vague.
> - https://news.ycombinator.com/item?id=11954988

## Courses

> - you can just take the courses for free
> - Is it possiblte, to complete those courses for free and get the nanodegree with a one month subscription?
> - nope, you have to be graded for the projects you complete as part of the degree
> https://www.reddit.com/r/MachineLearning/comments/3sebf0/udacity_machine_learning_engineer_nanodegree/

- https://www.quora.com/Is-the-Machine-Learning-Nano-degree-Udacity-worth-it-given-I-have-a-BS-in-Computer-Science
- http://www.learndatasci.com/best-data-science-online-courses/
- https://ask.slashdot.org/story/15/12/28/1745237/ask-slashdot-how-to-get-into-machine-learning
